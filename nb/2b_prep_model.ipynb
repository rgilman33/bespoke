{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b6bfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beans/bespoke/constants.py:448: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  propref_aux_target['ixx'] = list(range(len(propref_aux_target))) # dumb self ix so can grab sigmoid from within this smaller df\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(f'/home/beans/bespoke')\n",
    "\n",
    "from models import EffNet\n",
    "from constants import *\n",
    "from imports import *\n",
    "from loaders import TrnLoader\n",
    "from train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1474e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterates_to_combine = list(range(44, 49)) #[7, 8, 9, 10]\n",
    "model_stem = '1.18'\n",
    "#run_id = 'kszjvkvy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac313fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#glob.glob(f\"{BESPOKE_ROOT}/models/m_{model_stem}*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb1606e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/beans/bespoke/models/m1.18_e44.torch',\n",
       " '/home/beans/bespoke/models/m1.18_e45.torch',\n",
       " '/home/beans/bespoke/models/m1.18_e46.torch',\n",
       " '/home/beans/bespoke/models/m1.18_e47.torch',\n",
       " '/home/beans/bespoke/models/m1.18_e48.torch']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_paths = [f\"{BESPOKE_ROOT}/models/m{model_stem}_e{e}.torch\" for e in iterates_to_combine]\n",
    "m_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac429674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_dicts = [torch.load(p) for p in m_paths]\n",
    "state_dict_avg = state_dicts[0].copy() # placeholder for a sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ade8e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in state_dict_avg.keys():\n",
    "    state_dict_avg[k] = sum([sd[k] for sd in state_dicts]) / len(state_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f17a0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = EffNet().to(device)\n",
    "m.train()\n",
    "m.load_state_dict(state_dict_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b382621d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_UPDATES = 256\n",
    "for mm in m.modules():\n",
    "    if type(mm) == nn.BatchNorm2d: \n",
    "        mm.momentum = 1/N_UPDATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a82197f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beans/miniconda3/envs/py38/lib/python3.8/site-packages/albumentations/augmentations/transforms.py:1175: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n",
      "/home/beans/miniconda3/envs/py38/lib/python3.8/site-packages/albumentations/augmentations/transforms.py:316: FutureWarning: JpegCompression has been deprecated. Please use ImageCompression\n",
      "  warnings.warn(\n",
      "/home/beans/miniconda3/envs/py38/lib/python3.8/site-packages/albumentations/augmentations/transforms.py:1149: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got first chunk\n",
      "CPU times: user 872 ms, sys: 3.44 s, total: 4.31 s\n",
      "Wall time: 14 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/beans/miniconda3/envs/py38/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/beans/miniconda3/envs/py38/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/beans/bespoke/loaders.py\", line 105, in make_chunks\n",
      "    if get_loader_should_stop():\n",
      "  File \"/home/beans/bespoke/constants.py\", line 116, in get_loader_should_stop\n",
      "    return np.load(f\"{BESPOKE_ROOT}/tmp/trnloader_should_stop.npy\")[0]\n",
      "  File \"/home/beans/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/npyio.py\", line 417, in load\n",
      "    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BS = 64 # 128 got an error?\n",
    "dataloader = TrnLoader(bs=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71c48210",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(dataloader, m, model_stem=model_stem, opt=None, log_wandb=False, updates_per_epoch=N_UPDATES,\n",
    "                  backwards=False, total_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af166797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "\n",
      "\n",
      " {'avg_unc': -6.88952637, 'wp_angles': 0.00308067, 'wp_curvatures': 0.02676785, 'wp_headings': 0.00362837, 'wp_rolls': 0.23131371, 'wp_zs': 0.08237338, 'has_stop': 0.01137814, 'stop_dist': 0.00382139, 'has_lead': 0.00621726, 'lead_dist': 0.00248271, 'lead_speed': 0.19403276, 'dagger_shift': 0.02777812, 'lane_width': 0.0590353, 'rd_is_lined': 0.00086395, 'pitch': 7.57e-06, 'yaw': 7.28e-06, 'unc': 1.32972336, 'logistical/obs_consumed_per_second': 161.39370079, 'logistical/obs_generated_per_second': 78.0, 'logistical/slowest_runner_obs_per_sec': 5.0, 'logistical/data_consumption_ratio': 2.07129132, 'logistical/manual_train_pause': 0.0, 'timing/get batch from dataloader': 0.04270266, 'timing/model forward': 0.41074363, 'timing/calc losses': 0.00379752, 'timing/backwards': 3.67e-06, 'timing/get worst': 2.19e-06, 'timing/logging': 2.08e-06, 'timing/calc timing': 0.00320576, 'timing/trn update': 0.46046018, 'logistical/max_param': 32.69414902, 'logistical/lr': 0.0, 'logistical/mins_since_slowest_runner_reported': 22.06760958, 'logistical/snr': 0.0}\n",
      "\n",
      " {'timing/get chunk from queue': 6.16434347, 'timing/prep image': 0.23044702, 'timing/prep aux': 0.05405352, 'timing/calc wp targets': 0.01550089, 'timing/assemble mask': 0.07358911, 'timing/prep wps': 0.04350836, 'timing/get_batch_at_ix': 1.509e-05, 'timing/promote backup chunk': 0.02705322, 'timing/queue_batch': 0.44417137, 'timing/wait bc batch not ready': 0.02918288}\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "\n",
      "\n",
      " {'logistical/obs_consumed_per_second': 159.5390625, 'logistical/obs_generated_per_second': 78.0, 'logistical/slowest_runner_obs_per_sec': 5.0, 'logistical/data_consumption_ratio': 2.04753621, 'logistical/manual_train_pause': 0.0, 'timing/get batch from dataloader': 0.22548556, 'timing/model forward': 0.36157899, 'timing/calc losses': 0.00596004, 'timing/backwards': 4.22e-06, 'timing/get worst': 2.38e-06, 'timing/logging': 0.00202786, 'timing/calc timing': 0.00350088, 'timing/trn update': 0.59856271, 'avg_unc': -6.94796753, 'wp_angles': 0.00281543, 'wp_curvatures': 0.01959479, 'wp_headings': 0.00346334, 'wp_rolls': 0.16455221, 'wp_zs': 0.06622589, 'has_stop': 0.01298891, 'stop_dist': 0.00560217, 'has_lead': 0.0083094, 'lead_dist': 0.00258053, 'lead_speed': 0.49819708, 'dagger_shift': 0.04893592, 'lane_width': 0.06698966, 'rd_is_lined': 0.00078716, 'pitch': 7.15e-06, 'yaw': 5.21e-06, 'unc': 1.19405746, 'logistical/max_param': 32.69414902, 'logistical/lr': 0.0, 'logistical/mins_since_slowest_runner_reported': 23.346395, 'logistical/snr': 0.0}\n",
      "\n",
      " {'timing/wait bc batch not ready': 0.1546875, 'timing/prep image': 0.24897852, 'timing/prep aux': 0.05239399, 'timing/calc wp targets': 0.01338666, 'timing/assemble mask': 0.03602028, 'timing/prep wps': 0.0157952, 'timing/get_batch_at_ix': 1.167e-05, 'timing/promote backup chunk': 0.21020154, 'timing/queue_batch': 0.57679177, 'timing/get chunk from queue': 9.03580606}\n",
      "{'logistical/obs_consumed_per_second': 88.0, 'logistical/obs_generated_per_second': 78.0, 'logistical/slowest_runner_obs_per_sec': 5.0, 'logistical/data_consumption_ratio': 1.12309699, 'logistical/manual_train_pause': 0.0, 'timing/get batch from dataloader': 0.10098195, 'timing/model forward': 0.3736887, 'timing/calc losses': 0.00199485, 'timing/backwards': 2.86e-06, 'timing/get worst': 1.91e-06, 'timing/logging': 0.2546556, 'timing/calc timing': 0.00235105, 'timing/trn update': 0.73367929}\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "\n",
      "\n",
      " {'avg_unc': -6.94418335, 'wp_angles': 0.00298568, 'wp_curvatures': 0.02060956, 'wp_headings': 0.00341991, 'wp_rolls': 0.16622424, 'wp_zs': 0.06715059, 'has_stop': 0.00921008, 'stop_dist': 0.00421494, 'has_lead': 0.01160302, 'lead_dist': 0.00391184, 'lead_speed': 0.60979009, 'dagger_shift': 0.06274003, 'lane_width': 0.06316304, 'rd_is_lined': 0.00338674, 'pitch': 7.39e-06, 'yaw': 5.6e-06, 'unc': 1.29784775, 'logistical/obs_consumed_per_second': 160.12598425, 'logistical/obs_generated_per_second': 78.0, 'logistical/slowest_runner_obs_per_sec': 5.0, 'logistical/data_consumption_ratio': 2.05509475, 'logistical/manual_train_pause': 0.0, 'timing/get batch from dataloader': 0.2339435, 'timing/model forward': 0.36183192, 'timing/calc losses': 0.00564665, 'timing/backwards': 3.96e-06, 'timing/get worst': 2.22e-06, 'timing/logging': 2.08e-06, 'timing/calc timing': 0.00337307, 'timing/trn update': 0.60480609, 'logistical/max_param': 32.69414902, 'logistical/lr': 0.0, 'logistical/mins_since_slowest_runner_reported': 24.63836504, 'logistical/snr': 0.0}\n",
      "\n",
      " {'timing/wait bc batch not ready': 0.16015625, 'timing/prep image': 0.25124875, 'timing/prep aux': 0.05483093, 'timing/calc wp targets': 0.01259702, 'timing/assemble mask': 0.03365752, 'timing/prep wps': 0.014124, 'timing/get_batch_at_ix': 1.132e-05, 'timing/promote backup chunk': 0.21814056, 'timing/queue_batch': 0.58461389, 'timing/get chunk from queue': 9.17150116}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "\n",
      "\n",
      " {'logistical/obs_consumed_per_second': 158.3359375, 'logistical/obs_generated_per_second': 78.0, 'logistical/slowest_runner_obs_per_sec': 5.0, 'logistical/data_consumption_ratio': 2.03163656, 'logistical/manual_train_pause': 0.0, 'timing/get batch from dataloader': 0.22934535, 'timing/model forward': 0.36276523, 'timing/calc losses': 0.00534593, 'timing/backwards': 4.01e-06, 'timing/get worst': 2.34e-06, 'timing/logging': 0.00235293, 'timing/calc timing': 0.00347927, 'timing/trn update': 0.60329786, 'avg_unc': -6.93737793, 'wp_angles': 0.00238227, 'wp_curvatures': 0.01880404, 'wp_headings': 0.00308692, 'wp_rolls': 0.13779259, 'wp_zs': 0.07009721, 'has_stop': 0.01940705, 'stop_dist': 0.00721279, 'has_lead': 0.01495551, 'lead_dist': 0.00402111, 'lead_speed': 0.69331998, 'dagger_shift': 0.05666035, 'lane_width': 0.06069207, 'rd_is_lined': 0.00192888, 'pitch': 5.61e-06, 'yaw': 4.84e-06, 'unc': 1.16157913, 'logistical/max_param': 32.69414902, 'logistical/lr': 0.0, 'logistical/mins_since_slowest_runner_reported': 25.92545826, 'logistical/snr': 0.0}\n",
      "\n",
      " {'timing/wait bc batch not ready': 0.159375, 'timing/prep image': 0.24793284, 'timing/prep aux': 0.05549163, 'timing/calc wp targets': 0.01246527, 'timing/assemble mask': 0.03733902, 'timing/prep wps': 0.01426322, 'timing/get_batch_at_ix': 1.196e-05, 'timing/promote backup chunk': 0.21129012, 'timing/queue_batch': 0.57879803, 'timing/get chunk from queue': 9.13051987}\n",
      "{'logistical/obs_consumed_per_second': 98.0, 'logistical/obs_generated_per_second': 78.0, 'logistical/slowest_runner_obs_per_sec': 5.0, 'logistical/data_consumption_ratio': 1.25448757, 'logistical/manual_train_pause': 0.0, 'timing/get batch from dataloader': 0.00034237, 'timing/model forward': 0.36338687, 'timing/calc losses': 0.00498962, 'timing/backwards': 2.62e-06, 'timing/get worst': 1.67e-06, 'timing/logging': 0.28600359, 'timing/calc timing': 0.00269055, 'timing/trn update': 0.65742302}\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "\n",
      "\n",
      " {'avg_unc': -6.89422607, 'wp_angles': 0.00308257, 'wp_curvatures': 0.02378249, 'wp_headings': 0.00346049, 'wp_rolls': 0.18955326, 'wp_zs': 0.06003571, 'has_stop': 0.00831835, 'stop_dist': 0.00529855, 'has_lead': 0.00867387, 'lead_dist': 0.00408474, 'lead_speed': 0.62108302, 'dagger_shift': 0.0629499, 'lane_width': 0.06301785, 'rd_is_lined': 0.00072422, 'pitch': 6.38e-06, 'yaw': 5.42e-06, 'unc': 1.28328323, 'logistical/obs_consumed_per_second': 162.59055118, 'logistical/obs_generated_per_second': 78.0, 'logistical/slowest_runner_obs_per_sec': 5.0, 'logistical/data_consumption_ratio': 2.08631868, 'logistical/manual_train_pause': 0.0, 'timing/get batch from dataloader': 0.22875066, 'timing/model forward': 0.36244683, 'timing/calc losses': 0.00478852, 'timing/backwards': 4.05e-06, 'timing/get worst': 2.31e-06, 'timing/logging': 2.23e-06, 'timing/calc timing': 0.00327672, 'timing/trn update': 0.59927405, 'logistical/max_param': 32.69414902, 'logistical/lr': 0.0, 'logistical/mins_since_slowest_runner_reported': 27.2055798, 'logistical/snr': 0.0}\n",
      "\n",
      " {'timing/wait bc batch not ready': 0.15703125, 'timing/prep image': 0.25248159, 'timing/prep aux': 0.05373016, 'timing/calc wp targets': 0.01226364, 'timing/assemble mask': 0.03395538, 'timing/prep wps': 0.01348291, 'timing/get_batch_at_ix': 1.206e-05, 'timing/promote backup chunk': 0.21835602, 'timing/queue_batch': 0.58428559, 'timing/get chunk from queue': 9.07522133}\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n",
      "backup chunk not yet ready\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/bespoke/train_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ending training early\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bespoke/train_utils.py\u001b[0m in \u001b[0;36m_run_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get batch from dataloader\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwps_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_targets_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobsnet_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bespoke/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, aux)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_sequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint_sequential\u001b[0;34m(functions, segments, input, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msegment_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         input = checkpoint(run_function(start, end, functions), input,\n\u001b[0m\u001b[1;32m    323\u001b[0m                            preserve_rng_state=preserve)\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_reentrant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         return _checkpoint_without_reentrant(\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Point-wise linear projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_pwl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time \n",
    "with torch.no_grad():\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d5cc165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mm in m.modules():\n",
    "    if type(mm) == nn.BatchNorm2d: \n",
    "        mm.momentum = .1 # set this back to value used for train. Actually is this even necessary? is this stored in state_dict?\n",
    "        #print(mm.momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6b8eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m.state_dict(), f'{BESPOKE_ROOT}/models/m{model_stem}_avg.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01c81017",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model_stem = f\"{model_stem}_avg\" # rw evaluator uses this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d711583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrgilman33\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Resuming run <strong><a href=\"https://wandb.ai/rgilman33/carla/runs/1gdicwi9\" target=\"_blank\">rich-sky-365</a></strong> to <a href=\"https://wandb.ai/rgilman33/carla\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/rgilman33/carla/runs/1gdicwi9?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5cb26dfbb0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(id='1gdicwi9', project=\"carla\", resume=\"allow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72657ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.68 ms, sys: 15.4 s, total: 15.4 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from rollout import RwEvaluator\n",
    "rw_evaluator = RwEvaluator([\"run_555a\", \"run_556a\", \"run_556b\", \"run_556c\"], m, wandb=wandb, save_rollouts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16c77c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "loader is done\n",
      "Rollout complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beans/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning:\n",
      "\n",
      "None of the inputs have requires_grad=True. Gradients will be None\n",
      "\n",
      "/home/beans/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning:\n",
      "\n",
      "None of the inputs have requires_grad=True. Gradients will be None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n",
      "3200\n",
      "3200\n",
      "loader is done\n",
      "loader is done\n",
      "Rollout complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beans/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning:\n",
      "\n",
      "None of the inputs have requires_grad=True. Gradients will be None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollout complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beans/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning:\n",
      "\n",
      "None of the inputs have requires_grad=True. Gradients will be None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader is done\n",
      "Rollout complete!\n"
     ]
    }
   ],
   "source": [
    "rw_evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8eab78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
