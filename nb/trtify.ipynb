{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9778773",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(f'/home/beans/bespoke')\n",
    "\n",
    "from models import EffNet\n",
    "from constants import *\n",
    "from imports import *\n",
    "from train_utils import *\n",
    "\n",
    "torch.__version__, torch.cuda.device_count(), torch.cuda.get_device_name(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be31733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_tensorrt\n",
    "import timm\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "torch.hub._validate_not_a_forked_repo=lambda a,b,c: True\n",
    "\n",
    "#efficientnet_b0 = timm.create_model('efficientnet_b0',pretrained=True)\n",
    "#efficientnet = timm.create_model('efficientnet_b4',pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13fb6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = EffNet().to(device) # 13M params, 11.6M without RNN, \n",
    "sum([torch.numel(p) for p in m.parameters()]) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66277ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = \"1.29_avg\"\n",
    "\n",
    "m.load_state_dict(torch.load(f\"{BESPOKE_ROOT}/models_deploy/m{stem}.torch\"), strict=False)\n",
    "backbone = m.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbdb109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "def benchmark(model, input_shape=(1024, 3, 512, 512), dtype='fp32', nwarmup=50, nruns=1000):\n",
    "    input_data = torch.randn(input_shape)\n",
    "    input_data = input_data.to(\"cuda\")\n",
    "    if dtype=='fp16':\n",
    "        input_data = input_data.half()\n",
    "        \n",
    "    print(\"Warm up ...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(nwarmup):\n",
    "            features = model(input_data)\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"Start timing ...\")\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, nruns+1):\n",
    "            start_time = time.time()\n",
    "            pred_loc  = model(input_data)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            timings.append(end_time - start_time)\n",
    "            if i%10==0:\n",
    "                print('Iteration %d/%d, avg batch time %.2f ms'%(i, nruns, np.mean(timings)*1000))\n",
    "\n",
    "    print(\"Input shape:\", input_data.size())\n",
    "    print('Average throughput: %.2f images/second'%(input_shape[0]/np.mean(timings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CHANNELS_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, N_CHANNELS_MODEL, IMG_HEIGHT, IMG_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12aa8e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = backbone.eval().to(\"cuda\")\n",
    "benchmark(model, input_shape=input_shape, nruns=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ead6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, torch.randn(input_shape).to(\"cuda\"))\n",
    "benchmark(traced_model, input_shape=input_shape, nruns=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed9918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc5fb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trt_model = torch_tensorrt.compile(model, \n",
    "    inputs= [torch_tensorrt.Input(input_shape)],\n",
    "    enabled_precisions= { torch_tensorrt.dtype.half} # Run with FP16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae6ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c3d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(trt_model, f\"{BESPOKE_ROOT}/trt_models/backbone_trt.jit.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aba703",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(trt_model, input_shape=input_shape, nruns=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdef6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe try something w this bc was getting errors about mem \"CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f29e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46095cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e9591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(f'/home/beans/bespoke')\n",
    "\n",
    "from models import EffNet\n",
    "from constants import *\n",
    "from imports import *\n",
    "from train_utils import *\n",
    "\n",
    "import torch\n",
    "import torch_tensorrt\n",
    "import timm\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e723aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models.helpers:Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b4_ra2_320-7eb33cd5.pth)\n",
      "INFO:timm.models.helpers:Converted input conv conv_stem pretrained weights from 3 to 4 channel(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18297.787"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem = \"1.29_avg\"\n",
    "\n",
    "m = EffNet().to(device) \n",
    "m.load_state_dict(torch.load(f\"{BESPOKE_ROOT}/models_deploy/m{stem}.torch\"))\n",
    "m.model_stem = stem\n",
    "sum([torch.numel(p) for p in m.parameters()]) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ba8d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.5.0\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.5.0\n",
      "WARNING: [Torch-TensorRT] - CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n"
     ]
    }
   ],
   "source": [
    "m.load_trt_backbone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f54144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrgilman33\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Resuming run <strong><a href=\"https://wandb.ai/rgilman33/carla/runs/66i4vxlw\" target=\"_blank\">luminous-kumquat-373</a></strong> to <a href=\"https://wandb.ai/rgilman33/carla\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/rgilman33/carla/runs/66i4vxlw?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa3d9a772e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(id='66i4vxlw', project=\"carla\", resume=\"allow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4704a2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.95 ms, sys: 734 µs, total: 2.69 ms\n",
      "Wall time: 2.31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from rollout import RwEvaluator\n",
    "rw_evaluator = RwEvaluator(m, wandb=wandb, save_rollouts=False, trt=True, run_ids=[\"run_556a\"], bptt=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4020bcb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "loader is done\n",
      "Rollout complete!\n",
      "down w rollouts, reporting\n",
      "run_556a\n"
     ]
    }
   ],
   "source": [
    "rw_evaluator.evaluate() #20490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263386ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
